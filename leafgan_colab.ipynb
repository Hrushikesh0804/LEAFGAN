{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/d1.zip -d /content/d1\n"
      ],
      "metadata": {
        "id": "a8LqlET-7hBw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "BASE = \"https://unshielded-scalpless-ava.ngrok-free.dev\"\n",
        "r = requests.get(f\"{BASE}/ping\", headers={\"ngrok-skip-browser-warning\":\"true\"})\n",
        "print(r.status_code, r.text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Rqk3ZQpgtDJ",
        "outputId": "8ad56794-fe4d-4eda-80fd-9b27f28f39e8"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "200 {\"ok\":true}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "NGROK_API = \"https://unshielded-scalpless-ava.ngrok-free.dev\"  # your API URL\n",
        "\n",
        "# Upload a zip to start a job\n",
        "files = {\"file\": open(\"Apple_Dataset.zip\\content\\Apple_Dataset\", \"rb\")}\n",
        "r = requests.post(f\"{NGROK_API}/upload\", files=files)\n",
        "print(r.status_code, r.text)\n",
        "job_id = r.json()[\"jobId\"]\n",
        "\n",
        "# Poll status\n",
        "import time\n",
        "while True:\n",
        "    s = requests.get(f\"{NGROK_API}/jobs/{job_id}\").json()\n",
        "    print(s[\"status\"], s[\"progress\"], s.get(\"message\",\"\"))\n",
        "    if s[\"status\"] in (\"succeeded\",\"failed\"):\n",
        "        break\n",
        "    time.sleep(2)\n",
        "\n",
        "# Download result\n",
        "if s[\"status\"] == \"succeeded\":\n",
        "    z = requests.get(f\"{NGROK_API}/jobs/{job_id}/download\")\n",
        "    with open(\"/content/balanced_dataset.zip\", \"wb\") as f:\n",
        "        f.write(z.content)\n",
        "    print(\"Saved to /content/balanced_dataset.zip\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 293
        },
        "id": "TWw6n0VEkhSD",
        "outputId": "15225219-d2db-4ac9-806e-df31d5e0d73a"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<>:6: SyntaxWarning: invalid escape sequence '\\c'\n",
            "<>:6: SyntaxWarning: invalid escape sequence '\\c'\n",
            "/tmp/ipython-input-3128384832.py:6: SyntaxWarning: invalid escape sequence '\\c'\n",
            "  files = {\"file\": open(\"Apple_Dataset.zip\\content\\Apple_Dataset\", \"rb\")}\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'Apple_Dataset.zip\\\\content\\\\Apple_Dataset'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3128384832.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Upload a zip to start a job\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mfiles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"file\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Apple_Dataset.zip\\content\\Apple_Dataset\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{NGROK_API}/upload\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Apple_Dataset.zip\\\\content\\\\Apple_Dataset'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ✅ One-cell pipeline: build LeafGAN-style dataset, then make LFLSeg dataset from it\n",
        "\n",
        "import os, shutil, random\n",
        "from PIL import Image\n",
        "\n",
        "# --------------------\n",
        "# USER PATHS (edit if needed)\n",
        "# --------------------\n",
        "healthy_src = \"/content/d1/d1/Apple___healthy\"\n",
        "diseased_src = \"/content/d1/d1/Apple___Black_rot\"\n",
        "\n",
        "# Where step 1 writes (CycleGAN/LeafGAN-style structure)\n",
        "leafgan_root = \"/content/Apple_Dataset_for_user\"          # <— step 2 will read from here\n",
        "\n",
        "# Where step 2 writes (LFLSeg-style structure)\n",
        "lflseg_root = \"/content/lflseg_dataset_sample_for_user\"\n",
        "\n",
        "split_ratio = 0.8  # 80% train / 20% test\n",
        "\n",
        "# --------------------\n",
        "# Utils\n",
        "# --------------------\n",
        "def ensure_dirs(path_list):\n",
        "    for p in path_list:\n",
        "        os.makedirs(p, exist_ok=True)\n",
        "\n",
        "def list_images(src):\n",
        "    return [f for f in os.listdir(src) if f.lower().endswith((\".jpg\",\".png\",\".jpeg\"))]\n",
        "\n",
        "# --------------------\n",
        "# STEP 1: Build LeafGAN/CycleGAN dataset: trainA/testA (healthy), trainB/testB (diseased)\n",
        "# --------------------\n",
        "for sub in [\"trainA\", \"testA\", \"trainB\", \"testB\"]:\n",
        "    os.makedirs(os.path.join(leafgan_root, sub), exist_ok=True)\n",
        "\n",
        "def split_and_copy(src, dst_train, dst_test, ratio=0.8):\n",
        "    if not os.path.exists(src):\n",
        "        raise FileNotFoundError(f\"Source not found: {src}\")\n",
        "    files = list_images(src)\n",
        "    random.shuffle(files)\n",
        "    split_idx = int(len(files) * ratio)\n",
        "    train_files, test_files = files[:split_idx], files[split_idx:]\n",
        "\n",
        "    for f in train_files:\n",
        "        shutil.copy(os.path.join(src, f), os.path.join(dst_train, f))\n",
        "    for f in test_files:\n",
        "        shutil.copy(os.path.join(src, f), os.path.join(dst_test, f))\n",
        "\n",
        "    return len(train_files), len(test_files)\n",
        "\n",
        "trainA_count, testA_count = split_and_copy(\n",
        "    healthy_src,\n",
        "    os.path.join(leafgan_root, \"trainA\"),\n",
        "    os.path.join(leafgan_root, \"testA\"),\n",
        "    split_ratio\n",
        ")\n",
        "trainB_count, testB_count = split_and_copy(\n",
        "    diseased_src,\n",
        "    os.path.join(leafgan_root, \"trainB\"),\n",
        "    os.path.join(leafgan_root, \"testB\"),\n",
        "    split_ratio\n",
        ")\n",
        "\n",
        "print(\"✅ Step 1 complete. LeafGAN dataset at:\", leafgan_root)\n",
        "print(f\"Healthy → trainA: {trainA_count}, testA: {testA_count}\")\n",
        "print(f\"Diseased → trainB: {trainB_count}, testB: {testB_count}\")\n",
        "\n",
        "# --------------------\n",
        "# STEP 2: Prepare LFLSeg dataset from Step 1 output\n",
        "# Structure:\n",
        "#   lflseg_root/\n",
        "#     train/{full_leaf, partial_leaf, non_leaf}\n",
        "#     test/{full_leaf, partial_leaf, non_leaf}\n",
        "# --------------------\n",
        "def collect_full_leaf(src_dirs, dst):\n",
        "    os.makedirs(dst, exist_ok=True)\n",
        "    for src in src_dirs:\n",
        "        if os.path.exists(src):\n",
        "            for f in list_images(src):\n",
        "                shutil.copy(os.path.join(src, f), os.path.join(dst, f))\n",
        "\n",
        "def generate_partial_leaf(src_full, dst_partial):\n",
        "    os.makedirs(dst_partial, exist_ok=True)\n",
        "    for f in list_images(src_full):\n",
        "        in_path = os.path.join(src_full, f)\n",
        "        try:\n",
        "            with Image.open(in_path) as img:\n",
        "                img = img.convert(\"RGB\")\n",
        "                w, h = img.size\n",
        "                crops = [\n",
        "                    img.crop((0, 0, w//2, h//2)),        # top-left\n",
        "                    img.crop((w//2, 0, w, h//2)),        # top-right\n",
        "                    img.crop((0, h//2, w//2, h)),        # bottom-left\n",
        "                    img.crop((w//2, h//2, w, h))         # bottom-right\n",
        "                ]\n",
        "                stem, _ = os.path.splitext(f)\n",
        "                for i, c in enumerate(crops):\n",
        "                    c.save(os.path.join(dst_partial, f\"{stem}_patch{i}.jpg\"), quality=95)\n",
        "        except Exception as e:\n",
        "            print(\"Skipping (partial) due to error:\", in_path, e)\n",
        "\n",
        "def generate_non_leaf(src_full, dst_non_leaf, count=300):\n",
        "    os.makedirs(dst_non_leaf, exist_ok=True)\n",
        "    files = list_images(src_full)\n",
        "    random.shuffle(files)\n",
        "    picked = files[:min(count, len(files))]\n",
        "    for f in picked:\n",
        "        in_path = os.path.join(src_full, f)\n",
        "        try:\n",
        "            with Image.open(in_path) as img:\n",
        "                img = img.convert(\"RGB\")\n",
        "                w, h = img.size\n",
        "                if w < 8 or h < 8:\n",
        "                    continue\n",
        "                # random mid-sized crop\n",
        "                cw, ch = max(16, w // 4), max(16, h // 4)\n",
        "                x0 = random.randint(0, max(0, w - cw))\n",
        "                y0 = random.randint(0, max(0, h - ch))\n",
        "                crop = img.crop((x0, y0, x0 + cw, y0 + ch))\n",
        "                stem, _ = os.path.splitext(f)\n",
        "                crop.save(os.path.join(dst_non_leaf, f\"bg_{stem}.jpg\"), quality=95)\n",
        "        except Exception as e:\n",
        "            print(\"Skipping (non-leaf) due to error:\", in_path, e)\n",
        "\n",
        "for split in [\"train\", \"test\"]:\n",
        "    full_dst = os.path.join(lflseg_root, split, \"full_leaf\")\n",
        "    partial_dst = os.path.join(lflseg_root, split, \"partial_leaf\")\n",
        "    nonleaf_dst = os.path.join(lflseg_root, split, \"non_leaf\")\n",
        "\n",
        "    if split == \"train\":\n",
        "        collect_full_leaf(\n",
        "            [os.path.join(leafgan_root, \"trainA\"), os.path.join(leafgan_root, \"trainB\")],\n",
        "            full_dst\n",
        "        )\n",
        "    else:\n",
        "        collect_full_leaf(\n",
        "            [os.path.join(leafgan_root, \"testA\"), os.path.join(leafgan_root, \"testB\")],\n",
        "            full_dst\n",
        "        )\n",
        "\n",
        "    generate_partial_leaf(full_dst, partial_dst)\n",
        "    generate_non_leaf(full_dst, nonleaf_dst)\n",
        "\n",
        "print(\"✅ Step 2 complete. LFLSeg dataset at:\", lflseg_root)\n",
        "\n",
        "# Verify counts\n",
        "for split in [\"train\", \"test\"]:\n",
        "    for cls in [\"full_leaf\", \"partial_leaf\", \"non_leaf\"]:\n",
        "        path = os.path.join(lflseg_root, split, cls)\n",
        "        cnt = len([f for f in os.listdir(path) if f.lower().endswith((\".jpg\",\".png\",\".jpeg\"))]) if os.path.exists(path) else 0\n",
        "        print(f\"{split:5s} {cls:12s}: {cnt}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zKlPEEdm7Iom",
        "outputId": "abcf83cf-35cb-41ca-b7f5-e259126548a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Step 1 complete. LeafGAN dataset at: /content/Apple_Dataset_for_user\n",
            "Healthy → trainA: 1316, testA: 329\n",
            "Diseased → trainB: 496, testB: 125\n",
            "✅ Step 2 complete. LFLSeg dataset at: /content/lflseg_dataset_sample_for_user\n",
            "train full_leaf   : 1812\n",
            "train partial_leaf: 7248\n",
            "train non_leaf    : 300\n",
            "test  full_leaf   : 454\n",
            "test  partial_leaf: 1816\n",
            "test  non_leaf    : 300\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# (Optional) faster installs on fresh runtimes\n",
        "!pip install --quiet torch torchvision\n",
        "\n",
        "# =========================\n",
        "# LFLSEG training (3 epochs)\n",
        "# =========================\n",
        "import os, torch, json\n",
        "from torch import nn, optim\n",
        "from torchvision import transforms, datasets, models\n",
        "from torchvision.models import ResNet101_Weights\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Must match the output of your FIRST cell:\n",
        "data_root = \"/content/lflseg_dataset_sample_for_user\"\n",
        "train_dir = os.path.join(data_root, \"train\")\n",
        "val_dir   = os.path.join(data_root, \"test\")\n",
        "\n",
        "assert os.path.isdir(train_dir), f\"Train dir not found: {train_dir}\"\n",
        "assert os.path.isdir(val_dir),   f\"Val dir not found: {val_dir}\"\n",
        "\n",
        "# Data transforms\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.Resize(256),\n",
        "    transforms.RandomResizedCrop(224),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomRotation(degrees=(0, 270)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225])\n",
        "])\n",
        "\n",
        "transform_val = transforms.Compose([\n",
        "    transforms.Resize(256),\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225])\n",
        "])\n",
        "\n",
        "# Datasets / loaders\n",
        "train_ds = datasets.ImageFolder(train_dir, transform=transform_train)\n",
        "val_ds   = datasets.ImageFolder(val_dir,   transform=transform_val)\n",
        "\n",
        "# Save the class→index mapping for later inference\n",
        "os.makedirs(\"/content/ckpts\", exist_ok=True)\n",
        "with open(\"/content/ckpts/class_to_idx.json\", \"w\") as f:\n",
        "    json.dump(train_ds.class_to_idx, f, indent=2)\n",
        "\n",
        "num_workers = 2  # Colab-friendly\n",
        "batch_size  = 64\n",
        "\n",
        "train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True,  num_workers=num_workers, pin_memory=True)\n",
        "val_loader   = DataLoader(val_ds,   batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=True)\n",
        "\n",
        "# Model\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "weights = ResNet101_Weights.IMAGENET1K_V1\n",
        "model = models.resnet101(weights=weights)\n",
        "\n",
        "# Replace classifier head (3 classes: full_leaf, partial_leaf, non_leaf)\n",
        "num_classes = len(train_ds.classes)\n",
        "model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
        "model = model.to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=1e-3, momentum=0.9, weight_decay=1e-4)\n",
        "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
        "\n",
        "# -----------------\n",
        "# Train for 3 epochs\n",
        "# -----------------\n",
        "best_acc = 0.0\n",
        "epochs = 3\n",
        "\n",
        "for epoch in range(1, epochs + 1):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    running_correct = 0\n",
        "\n",
        "    for imgs, labels in train_loader:\n",
        "        imgs, labels = imgs.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        out = model(imgs)\n",
        "        loss = criterion(out, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item() * imgs.size(0)\n",
        "        running_correct += (out.argmax(1) == labels).sum().item()\n",
        "\n",
        "    scheduler.step()\n",
        "\n",
        "    # Validation\n",
        "    model.eval()\n",
        "    correct = 0; total = 0\n",
        "    with torch.no_grad():\n",
        "        for imgs, labels in val_loader:\n",
        "            imgs, labels = imgs.to(device), labels.to(device)\n",
        "            out = model(imgs)\n",
        "            correct += (out.argmax(1) == labels).sum().item()\n",
        "            total += imgs.size(0)\n",
        "\n",
        "    train_loss = running_loss / len(train_ds) if len(train_ds) else 0.0\n",
        "    val_acc = (correct / total) if total else 0.0\n",
        "    print(f\"Epoch {epoch}/{epochs}: train_loss={train_loss:.4f}  val_acc={val_acc:.4f}\")\n",
        "\n",
        "    if val_acc > best_acc:\n",
        "        best_acc = val_acc\n",
        "        torch.save(model.state_dict(), \"/content/ckpts/lflseg_resnet101_best_for_user.pth\")\n",
        "\n",
        "print(\"✅ Done. Best val acc:\", best_acc)\n",
        "print(\"Saved:\", \"/content/ckpts/lflseg_resnet101_best_for_user.pth\")\n",
        "print(\"Classes:\", train_ds.classes)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gLARULL09wUk",
        "outputId": "4cd7b954-4ecd-4c79-d69e-05d79196a1bb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet101-63fe2227.pth\" to /root/.cache/torch/hub/checkpoints/resnet101-63fe2227.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 171M/171M [00:00<00:00, 182MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3: train_loss=0.3062  val_acc=0.8755\n",
            "Epoch 2/3: train_loss=0.1571  val_acc=0.8895\n",
            "Epoch 3/3: train_loss=0.1002  val_acc=0.9074\n",
            "✅ Done. Best val acc: 0.9073929961089494\n",
            "Saved: /content/ckpts/lflseg_resnet101_best_for_user.pth\n",
            "Classes: ['full_leaf', 'non_leaf', 'partial_leaf']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Installs (pytorch-grad-cam is the correct package name)\n",
        "!pip install --quiet grad-cam opencv-python pillow tqdm\n",
        "\n",
        "\n",
        "# ---- Grad-CAM over LeafGAN-style splits, saves masks + masked RGB ----\n",
        "import os, cv2, numpy as np, torch, json\n",
        "from pathlib import Path\n",
        "from PIL import Image\n",
        "from torchvision import transforms\n",
        "from torchvision.models import resnet101\n",
        "from pytorch_grad_cam import GradCAM\n",
        "from pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget\n",
        "from tqdm import tqdm\n",
        "\n",
        "# ====== CONFIG (paths aligned to your previous cells) ======\n",
        "# 1) Trained LFLSeg weights from your SECOND cell:\n",
        "LFLSeg_weights = \"/content/ckpts/lflseg_resnet101_best_for_user.pth\"\n",
        "\n",
        "# 2) Root of original LeafGAN/CycleGAN dataset created in your FIRST cell:\n",
        "#    (contains trainA, trainB, testA, testB)\n",
        "original_root = \"/content/Apple_Dataset_for_user\"\n",
        "\n",
        "# 3) Output root (masks + masked RGBs will be written here)\n",
        "out_root = \"/content/masked_apple_dataset\"\n",
        "os.makedirs(out_root, exist_ok=True)\n",
        "\n",
        "# 4) LFLSeg dataset root from FIRST cell (for class auto-detect: 'full_leaf', 'partial_leaf', 'non_leaf')\n",
        "lflseg_root = \"/content/lflseg_dataset_sample_for_user\"\n",
        "\n",
        "# 5) Splits to process\n",
        "splits = [\"trainA\", \"trainB\", \"testA\", \"testB\"]\n",
        "\n",
        "# GradCAM / mask hyperparams\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "FULL_LEAF_CLASS_INDEX = None   # auto-detect from lflseg dataset\n",
        "THRESH_PCT = 80\n",
        "GAUSSIAN_SIGMA = 3.0\n",
        "MORPH_KERNEL = 3\n",
        "MIN_AREA_RATIO = 0.001\n",
        "\n",
        "# ====== helpers ======\n",
        "def safe_load_state(model, path, map_location):\n",
        "    ckpt = torch.load(path, map_location=map_location)\n",
        "    if isinstance(ckpt, dict) and \"state_dict\" in ckpt:\n",
        "        ckpt = ckpt[\"state_dict\"]\n",
        "    if isinstance(ckpt, dict):\n",
        "        new_ckpt = {}\n",
        "        for k, v in ckpt.items():\n",
        "            new_k = k.replace(\"module.\", \"\") if k.startswith(\"module.\") else k\n",
        "            new_ckpt[new_k] = v\n",
        "        ckpt = new_ckpt\n",
        "    return ckpt\n",
        "\n",
        "def green_mask_from_rgb(rgb_img):\n",
        "    hsv = cv2.cvtColor(rgb_img, cv2.COLOR_RGB2HSV)\n",
        "    lower = np.array([20, 25, 20])\n",
        "    upper = np.array([100, 255, 255])\n",
        "    mask = cv2.inRange(hsv, lower, upper)\n",
        "    return (mask > 0).astype(np.uint8)\n",
        "\n",
        "def fill_holes_and_keep_largest(mask):\n",
        "    kernel_close = np.ones((7,7), np.uint8)\n",
        "    mask_close = cv2.morphologyEx(mask.astype(np.uint8), cv2.MORPH_CLOSE, kernel_close)\n",
        "    num_labels, labels, stats, _ = cv2.connectedComponentsWithStats(mask_close, connectivity=8)\n",
        "    if num_labels <= 1:\n",
        "        final = mask_close\n",
        "    else:\n",
        "        areas = stats[1:, cv2.CC_STAT_AREA]\n",
        "        largest_idx = 1 + int(np.argmax(areas))\n",
        "        final = (labels == largest_idx).astype(np.uint8)\n",
        "    k2 = np.ones((MORPH_KERNEL, MORPH_KERNEL), np.uint8)\n",
        "    final = cv2.morphologyEx(final, cv2.MORPH_OPEN, k2)\n",
        "    final = cv2.morphologyEx(final, cv2.MORPH_CLOSE, k2)\n",
        "    return final\n",
        "\n",
        "# ====== Load model & GradCAM ======\n",
        "print(\"Loading LFLSeg model...\")\n",
        "model = resnet101(weights=None)\n",
        "model.fc = torch.nn.Linear(model.fc.in_features, 3)   # 3 classes: full/partial/non_leaf\n",
        "state = safe_load_state(model, LFLSeg_weights, map_location=torch.device(DEVICE))\n",
        "model.load_state_dict(state)\n",
        "model = model.to(DEVICE).eval()\n",
        "target_layer = model.layer4[-1]\n",
        "cam = GradCAM(model=model, target_layers=[target_layer])\n",
        "\n",
        "# ====== auto-detect FULL_LEAF_CLASS_INDEX from lflseg dataset ======\n",
        "if FULL_LEAF_CLASS_INDEX is None:\n",
        "    try:\n",
        "        from torchvision import datasets\n",
        "        ds = datasets.ImageFolder(os.path.join(lflseg_root, \"train\"))\n",
        "        print(\"Detected LFLSeg classes:\", ds.classes, ds.class_to_idx)\n",
        "        if \"full_leaf\" in ds.class_to_idx:\n",
        "            FULL_LEAF_CLASS_INDEX = ds.class_to_idx[\"full_leaf\"]\n",
        "        else:\n",
        "            FULL_LEAF_CLASS_INDEX = 0\n",
        "            print(\"Warning: 'full_leaf' not found; defaulting to index 0\")\n",
        "    except Exception as e:\n",
        "        FULL_LEAF_CLASS_INDEX = 0\n",
        "        print(\"Auto-detect failed; defaulting FULL_LEAF_CLASS_INDEX=0. Error:\", e)\n",
        "\n",
        "# ====== preprocessing (match validation transforms) ======\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize(256),\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n",
        "])\n",
        "\n",
        "# ====== Process each split ======\n",
        "total_processed = 0\n",
        "for split in splits:\n",
        "    orig_dir = os.path.join(original_root, split)\n",
        "    save_mask_dir = os.path.join(out_root, split, \"masks\")\n",
        "    save_rgb_dir  = os.path.join(out_root, split, \"masked_rgb\")\n",
        "    os.makedirs(save_mask_dir, exist_ok=True)\n",
        "    os.makedirs(save_rgb_dir, exist_ok=True)\n",
        "\n",
        "    if not os.path.exists(orig_dir):\n",
        "        print(f\"Skipping {split}: directory not found -> {orig_dir}\")\n",
        "        continue\n",
        "\n",
        "    files = sorted([f for f in os.listdir(orig_dir) if f.lower().endswith((\".jpg\",\".png\",\".jpeg\"))])\n",
        "    print(f\"Processing {len(files)} images in {split} -> {orig_dir}\")\n",
        "\n",
        "    for fname in tqdm(files):\n",
        "        orig_path = os.path.join(orig_dir, fname)\n",
        "        try:\n",
        "            pil = Image.open(orig_path).convert(\"RGB\")\n",
        "        except Exception as e:\n",
        "            print(\"Failed to open\", orig_path, e)\n",
        "            continue\n",
        "        rgb = np.array(pil)\n",
        "        h, w = rgb.shape[:2]\n",
        "\n",
        "        # prepare input for CAM\n",
        "        inp = transform(pil).unsqueeze(0).to(DEVICE)\n",
        "\n",
        "        # GradCAM targeting full_leaf\n",
        "        targets = [ClassifierOutputTarget(FULL_LEAF_CLASS_INDEX)]\n",
        "        grayscale_cam = cam(input_tensor=inp, targets=targets)[0]  # 224x224\n",
        "\n",
        "        # smooth + threshold\n",
        "        cam_uint8 = (grayscale_cam * 255).astype(np.uint8)\n",
        "        cam_blur = cv2.GaussianBlur(cam_uint8, (0,0), GAUSSIAN_SIGMA)\n",
        "        cam_float = cam_blur.astype(np.float32) / 255.0\n",
        "        try:\n",
        "            _, th = cv2.threshold((cam_float*255).astype(np.uint8), 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
        "            cam_mask = (th > 0).astype(np.uint8)\n",
        "        except Exception:\n",
        "            p = np.percentile(cam_float, THRESH_PCT)\n",
        "            cam_mask = (cam_float >= p).astype(np.uint8)\n",
        "\n",
        "        # resize to original size\n",
        "        cam_mask_resized = cv2.resize(cam_mask, (w, h), interpolation=cv2.INTER_NEAREST)\n",
        "\n",
        "        # refine with green color mask\n",
        "        green = green_mask_from_rgb(rgb)\n",
        "        combined = cam_mask_resized & green\n",
        "        if combined.sum() < max(1, int(MIN_AREA_RATIO * (h*w))):\n",
        "            combined = cam_mask_resized.copy()\n",
        "\n",
        "        # keep largest, fill holes\n",
        "        final_mask = fill_holes_and_keep_largest(combined)\n",
        "\n",
        "        # fallback if too small/too big\n",
        "        area = final_mask.sum()\n",
        "        img_area = h * w\n",
        "        if area < max(1, int(MIN_AREA_RATIO * img_area)) or area > 0.98 * img_area:\n",
        "            final_mask = fill_holes_and_keep_largest(cam_mask_resized)\n",
        "\n",
        "        # save mask + masked rgb\n",
        "        final_mask_u8 = (final_mask * 255).astype(np.uint8)\n",
        "        mask_out_path = os.path.join(save_mask_dir, Path(fname).stem + \"_mask.png\")\n",
        "        cv2.imwrite(mask_out_path, final_mask_u8)\n",
        "\n",
        "        masked_rgb = rgb.copy()\n",
        "        masked_rgb[final_mask == 0] = 0\n",
        "        masked_out_path = os.path.join(save_rgb_dir, Path(fname).stem + \"_masked.png\")\n",
        "        cv2.imwrite(masked_out_path, cv2.cvtColor(masked_rgb, cv2.COLOR_RGB2BGR))\n",
        "\n",
        "        total_processed += 1\n",
        "\n",
        "print(f\"\\n✅ Done. Processed {total_processed} images.\")\n",
        "print(\"Masks + masked RGB saved under:\", out_root)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wza9Rtr6CSNy",
        "outputId": "43dac584-09d2-4201-e514-5c786f51c52e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading LFLSeg model...\n",
            "Detected LFLSeg classes: ['full_leaf', 'non_leaf', 'partial_leaf'] {'full_leaf': 0, 'non_leaf': 1, 'partial_leaf': 2}\n",
            "Processing 1316 images in trainA -> /content/Apple_Dataset_for_user/trainA\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1316/1316 [01:02<00:00, 20.98it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing 496 images in trainB -> /content/Apple_Dataset_for_user/trainB\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 496/496 [00:21<00:00, 23.28it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing 329 images in testA -> /content/Apple_Dataset_for_user/testA\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 329/329 [00:14<00:00, 22.44it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing 125 images in testB -> /content/Apple_Dataset_for_user/testB\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 125/125 [00:05<00:00, 24.54it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "✅ Done. Processed 2266 images.\n",
            "Masks + masked RGB saved under: /content/masked_apple_dataset\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prepare_leafgan_dataset.py\n",
        "\n",
        "import os, shutil, random\n",
        "from pathlib import Path\n",
        "\n",
        "random.seed(42)\n",
        "\n",
        "# ✅ Path to GradCAM output\n",
        "masked_root = \"/content/masked_apple_dataset\"\n",
        "\n",
        "# ✅ Final output for GAN training\n",
        "leafgan_root = \"/content/LeafGAN_masked_final_dataset_for_user\"\n",
        "os.makedirs(leafgan_root, exist_ok=True)\n",
        "\n",
        "for d in [\"trainA\",\"trainB\",\"testA\",\"testB\"]:\n",
        "    os.makedirs(os.path.join(leafgan_root, d), exist_ok=True)\n",
        "\n",
        "def list_masked_rgb(split):\n",
        "    p = os.path.join(masked_root, split, \"masked_rgb\")\n",
        "    if not os.path.exists(p):\n",
        "        return []\n",
        "    return sorted([\n",
        "        os.path.join(p,f)\n",
        "        for f in os.listdir(p)\n",
        "        if f.lower().endswith((\".png\",\".jpg\",\".jpeg\"))\n",
        "    ])\n",
        "\n",
        "trainA_files = list_masked_rgb(\"trainA\")\n",
        "trainB_files = list_masked_rgb(\"trainB\")\n",
        "testA_files  = list_masked_rgb(\"testA\")\n",
        "testB_files  = list_masked_rgb(\"testB\")\n",
        "\n",
        "def ensure_test_split(train_list, test_list, dst_train, dst_test):\n",
        "    if len(test_list) == 0:\n",
        "        random.shuffle(train_list)\n",
        "        idx = int(0.8 * len(train_list))\n",
        "        train_part = train_list[:idx]\n",
        "        test_part  = train_list[idx:]\n",
        "    else:\n",
        "        train_part = train_list\n",
        "        test_part = test_list\n",
        "\n",
        "    for src in train_part:\n",
        "        shutil.copy(src, os.path.join(dst_train, os.path.basename(src)))\n",
        "    for src in test_part:\n",
        "        shutil.copy(src, os.path.join(dst_test, os.path.basename(src)))\n",
        "\n",
        "    return len(train_part), len(test_part)\n",
        "\n",
        "tA_train,tA_test = ensure_test_split(trainA_files, testA_files,\n",
        "                                     os.path.join(leafgan_root,\"trainA\"),\n",
        "                                     os.path.join(leafgan_root,\"testA\"))\n",
        "\n",
        "tB_train,tB_test = ensure_test_split(trainB_files, testB_files,\n",
        "                                     os.path.join(leafgan_root,\"trainB\"),\n",
        "                                     os.path.join(leafgan_root,\"testB\"))\n",
        "\n",
        "print(\"✅ LeafGAN dataset built at:\", leafgan_root)\n",
        "print(\"trainA:\", tA_train, \" testA:\", tA_test)\n",
        "print(\"trainB:\", tB_train, \" testB:\", tB_test)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K245XxDRD1BG",
        "outputId": "7a015d15-b608-423c-b9db-f1020b5fd2a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ LeafGAN dataset built at: /content/LeafGAN_masked_final_dataset_for_user\n",
            "trainA: 1316  testA: 329\n",
            "trainB: 496  testB: 125\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "from google.colab import files\n",
        "\n",
        "# Path to your GAN dataset\n",
        "leafgan_root = \"/content/LeafGAN_masked_final_dataset_for_user\"\n",
        "\n",
        "# Zip the dataset\n",
        "zip_path = \"/content/LeafGAN_masked_final_dataset.zip\"\n",
        "if os.path.exists(zip_path):\n",
        "    os.remove(zip_path)\n",
        "shutil.make_archive(\"/content/LeafGAN_masked_final_dataset\", 'zip', leafgan_root)\n",
        "\n",
        "print(\"✅ Zipped dataset at:\", zip_path)\n",
        "\n",
        "# Download to local machine\n",
        "files.download(zip_path)\n"
      ],
      "metadata": {
        "id": "goL2k1uNRujK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "90632c68-4dc3-4537-ad42-3d559b3a18f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Zipped dataset at: /content/LeafGAN_masked_final_dataset.zip\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_0184d2e6-1bcd-4407-b46e-92a0b51fecd0\", \"LeafGAN_masked_final_dataset.zip\", 121340861)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Colab cell\n",
        "!pip install torch torchvision tqdm pillow opencv-python pytorch-grad-cam\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "eIFIuBrdwz4o",
        "outputId": "db4c834a-0366-4f5b-ce6a-2500343f3442"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (0.23.0+cu126)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (4.67.1)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.12/dist-packages (11.3.0)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.12/dist-packages (4.12.0.88)\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement pytorch-grad-cam (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for pytorch-grad-cam\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# LeafGAN_train_no_masks.py  (paste into one notebook cell and run)\n",
        "!pip install --quiet grad-cam\n",
        "\n",
        "import os, time, itertools, random\n",
        "from pathlib import Path\n",
        "import torch, torch.nn as nn, torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms, utils\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "\n",
        "# ----------------- CONFIG -----------------\n",
        "LEAFGAN_ROOT = \"/content/LeafGAN_masked_final_dataset_for_user\"   # must contain trainA, trainB, testA, testB\n",
        "SAVE_ROOT    = \"/content/leafgan_checkpoints_no_masks_for_user\"\n",
        "os.makedirs(SAVE_ROOT, exist_ok=True)\n",
        "\n",
        "IMG_SIZE = 256\n",
        "BATCH_SIZE = 1\n",
        "EPOCHS = 2              # change as needed\n",
        "LR = 2e-4\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# Loss weights\n",
        "LAMBDA_CYCLE = 10.0\n",
        "LAMBDA_ID = 5.0\n",
        "\n",
        "# Random seed\n",
        "random.seed(42)\n",
        "# ------------------------------------------\n",
        "\n",
        "# ---------- Dataset ----------\n",
        "class LeafGANDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Loads unpaired CycleGAN-style images from trainA and trainB folders.\n",
        "    \"\"\"\n",
        "    def __init__(self, rootA, rootB, transform=None):\n",
        "        self.rootA = rootA\n",
        "        self.rootB = rootB\n",
        "        self.A_files = sorted([f for f in os.listdir(rootA) if f.lower().endswith((\".png\",\".jpg\",\".jpeg\"))])\n",
        "        self.B_files = sorted([f for f in os.listdir(rootB) if f.lower().endswith((\".png\",\".jpg\",\".jpeg\"))])\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return max(len(self.A_files), len(self.B_files))\n",
        "\n",
        "    def _load_img(self, path):\n",
        "        img = Image.open(path).convert(\"RGB\")\n",
        "        return img\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        a_name = self.A_files[idx % len(self.A_files)]\n",
        "        b_name = self.B_files[idx % len(self.B_files)]\n",
        "        A = self._load_img(os.path.join(self.rootA, a_name))\n",
        "        B = self._load_img(os.path.join(self.rootB, b_name))\n",
        "        if self.transform:\n",
        "            A_t = self.transform(A)\n",
        "            B_t = self.transform(B)\n",
        "        else:\n",
        "            A_t = A; B_t = B\n",
        "        return {\"A\": A_t, \"B\": B_t, \"A_name\": a_name, \"B_name\": b_name}\n",
        "\n",
        "# transforms\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,0.5,0.5),(0.5,0.5,0.5))\n",
        "])\n",
        "\n",
        "train_dataset = LeafGANDataset(os.path.join(LEAFGAN_ROOT,\"trainA\"),\n",
        "                               os.path.join(LEAFGAN_ROOT,\"trainB\"),\n",
        "                               transform=transform)\n",
        "dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2, drop_last=True)\n",
        "\n",
        "# ---------- Models ----------\n",
        "class ResnetBlock(nn.Module):\n",
        "    def __init__(self, dim):\n",
        "        super().__init__()\n",
        "        self.conv_block = nn.Sequential(\n",
        "            nn.ReflectionPad2d(1),\n",
        "            nn.Conv2d(dim, dim, 3),\n",
        "            nn.InstanceNorm2d(dim),\n",
        "            nn.ReLU(True),\n",
        "            nn.ReflectionPad2d(1),\n",
        "            nn.Conv2d(dim, dim, 3),\n",
        "            nn.InstanceNorm2d(dim),\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return x + self.conv_block(x)\n",
        "\n",
        "class ResnetGenerator(nn.Module):\n",
        "    def __init__(self, in_c=3, out_c=3, n_blocks=6, ngf=64):\n",
        "        super().__init__()\n",
        "        model = [nn.ReflectionPad2d(3),\n",
        "                 nn.Conv2d(in_c, ngf, 7), nn.InstanceNorm2d(ngf), nn.ReLU(True)]\n",
        "        # downsample\n",
        "        n_down = 2\n",
        "        curr = ngf\n",
        "        for i in range(n_down):\n",
        "            model += [nn.Conv2d(curr, curr*2, 3, 2, 1), nn.InstanceNorm2d(curr*2), nn.ReLU(True)]\n",
        "            curr *= 2\n",
        "        # resblocks\n",
        "        for _ in range(n_blocks):\n",
        "            model += [ResnetBlock(curr)]\n",
        "        # upsample\n",
        "        for i in range(n_down):\n",
        "            model += [nn.ConvTranspose2d(curr, curr//2, 3, 2, 1, output_padding=1), nn.InstanceNorm2d(curr//2), nn.ReLU(True)]\n",
        "            curr = curr//2\n",
        "        model += [nn.ReflectionPad2d(3), nn.Conv2d(curr, out_c, 7), nn.Tanh()]\n",
        "        self.model = nn.Sequential(*model)\n",
        "    def forward(self, x): return self.model(x)\n",
        "\n",
        "class PatchDiscriminator(nn.Module):\n",
        "    def __init__(self, in_c=3, ndf=64):\n",
        "        super().__init__()\n",
        "        model = [nn.Conv2d(in_c, ndf, 4, 2, 1), nn.LeakyReLU(0.2, True)]\n",
        "        curr = ndf\n",
        "        for n in [128,256,512]:\n",
        "            model += [nn.Conv2d(curr, n, 4, 2 if n<512 else 1, 1), nn.InstanceNorm2d(n), nn.LeakyReLU(0.2, True)]\n",
        "            curr = n\n",
        "        model += [nn.Conv2d(curr, 1, 4, 1, 1)]\n",
        "        self.model = nn.Sequential(*model)\n",
        "    def forward(self, x): return self.model(x)\n",
        "\n",
        "# Init models\n",
        "G_AB = ResnetGenerator().to(DEVICE)\n",
        "G_BA = ResnetGenerator().to(DEVICE)\n",
        "D_A = PatchDiscriminator().to(DEVICE)\n",
        "D_B = PatchDiscriminator().to(DEVICE)\n",
        "\n",
        "# weight init\n",
        "def init_weights(net, init_type='normal', gain=0.02):\n",
        "    def init_func(m):\n",
        "        classname = m.__class__.__name__\n",
        "        if hasattr(m, 'weight') and (classname.find('Conv')!=-1 or classname.find('Linear')!=-1):\n",
        "            if init_type == 'normal':\n",
        "                nn.init.normal_(m.weight.data, 0.0, gain)\n",
        "            elif init_type == 'xavier':\n",
        "                nn.init.xavier_normal_(m.weight.data)\n",
        "            if hasattr(m, 'bias') and m.bias is not None:\n",
        "                nn.init.constant_(m.bias.data, 0.0)\n",
        "        elif classname.find('BatchNorm2d') != -1 or classname.find('InstanceNorm2d') != -1:\n",
        "            if hasattr(m, 'weight') and m.weight is not None:\n",
        "                nn.init.normal_(m.weight.data, 1.0, gain)\n",
        "                nn.init.constant_(m.bias.data, 0.0)\n",
        "    net.apply(init_func)\n",
        "\n",
        "init_weights(G_AB); init_weights(G_BA); init_weights(D_A); init_weights(D_B)\n",
        "\n",
        "# ---------- Losses & optimizers ----------\n",
        "mse = nn.MSELoss()\n",
        "l1 = nn.L1Loss()\n",
        "opt_G = optim.Adam(itertools.chain(G_AB.parameters(), G_BA.parameters()), lr=LR, betas=(0.5,0.999))\n",
        "opt_D_A = optim.Adam(D_A.parameters(), lr=LR, betas=(0.5,0.999))\n",
        "opt_D_B = optim.Adam(D_B.parameters(), lr=LR, betas=(0.5,0.999))\n",
        "\n",
        "# For patch target shapes compute once by forward dummy\n",
        "with torch.no_grad():\n",
        "    dummy = torch.randn(1,3,IMG_SIZE,IMG_SIZE).to(DEVICE)\n",
        "    out = D_A(dummy)\n",
        "patch_h, patch_w = out.shape[2], out.shape[3]\n",
        "\n",
        "def patch_target(x, val):\n",
        "    return torch.full((x.size(0),1,patch_h,patch_w), val, device=DEVICE)\n",
        "\n",
        "# ---------- Training loop ----------\n",
        "print(\"Starting training on\", DEVICE)\n",
        "iters = 0\n",
        "save_every = 5  # epochs\n",
        "for epoch in range(1, EPOCHS+1):\n",
        "    epoch_start = time.time()\n",
        "    for i, data in enumerate(dataloader):\n",
        "        real_A = data[\"A\"].to(DEVICE)\n",
        "        real_B = data[\"B\"].to(DEVICE)\n",
        "\n",
        "        # ground truths\n",
        "        valid = patch_target(real_A, 1.0)\n",
        "        fake_label = patch_target(real_A, 0.0)\n",
        "\n",
        "        # ------------------ Train Generators ------------------\n",
        "        opt_G.zero_grad()\n",
        "        # generate\n",
        "        fake_B = G_AB(real_A)\n",
        "        fake_A = G_BA(real_B)\n",
        "        # adv loss\n",
        "        loss_GAN_AB = mse(D_B(fake_B), valid)\n",
        "        loss_GAN_BA = mse(D_A(fake_A), valid)\n",
        "        # cycle\n",
        "        rec_A = G_BA(fake_B)\n",
        "        rec_B = G_AB(fake_A)\n",
        "        loss_cycle = l1(rec_A, real_A) + l1(rec_B, real_B)\n",
        "        # identity\n",
        "        loss_id = l1(G_BA(real_A), real_A) + l1(G_AB(real_B), real_B)\n",
        "\n",
        "        loss_G = loss_GAN_AB + loss_GAN_BA + LAMBDA_CYCLE * loss_cycle + LAMBDA_ID * loss_id\n",
        "\n",
        "        loss_G.backward(); opt_G.step()\n",
        "\n",
        "        # ------------------ Train Discriminator A ------------------\n",
        "        opt_D_A.zero_grad()\n",
        "        loss_real = mse(D_A(real_A), valid)\n",
        "        loss_fake = mse(D_A(fake_A.detach()), fake_label)\n",
        "        loss_D_A = 0.5 * (loss_real + loss_fake)\n",
        "        loss_D_A.backward(); opt_D_A.step()\n",
        "\n",
        "        # ------------------ Train Discriminator B ------------------\n",
        "        opt_D_B.zero_grad()\n",
        "        loss_real = mse(D_B(real_B), valid)\n",
        "        loss_fake = mse(D_B(fake_B.detach()), fake_label)\n",
        "        loss_D_B = 0.5 * (loss_real + loss_fake)\n",
        "        loss_D_B.backward(); opt_D_B.step()\n",
        "\n",
        "        iters += 1\n",
        "\n",
        "    # end epoch\n",
        "    print(f\"[Epoch {epoch}/{EPOCHS}] loss_G:{loss_G.item():.4f} loss_D_A:{loss_D_A.item():.4f} loss_D_B:{loss_D_B.item():.4f} time:{time.time()-epoch_start:.1f}s\")\n",
        "\n",
        "    # save samples and checkpoints\n",
        "    if epoch % save_every == 0 or epoch==1:\n",
        "        # save sample images from a small batch\n",
        "        with torch.no_grad():\n",
        "            sample = next(iter(dataloader))\n",
        "            A_sample = sample[\"A\"].to(DEVICE)\n",
        "            B_sample = sample[\"B\"].to(DEVICE)\n",
        "            fakeB_sample = G_AB(A_sample)\n",
        "            fakeA_sample = G_BA(B_sample)\n",
        "            # denorm [-1,1] -> [0,1]\n",
        "            def denorm(x): return (x*0.5 + 0.5).clamp(0,1)\n",
        "            utils.save_image(denorm(fakeB_sample), os.path.join(SAVE_ROOT, f\"fakeB_epoch{epoch}.png\"), nrow=4)\n",
        "            utils.save_image(denorm(fakeA_sample), os.path.join(SAVE_ROOT, f\"fakeA_epoch{epoch}.png\"), nrow=4)\n",
        "        torch.save(G_AB.state_dict(), os.path.join(SAVE_ROOT, f\"G_AB_epoch{epoch}.pth\"))\n",
        "        torch.save(G_BA.state_dict(), os.path.join(SAVE_ROOT, f\"G_BA_epoch{epoch}.pth\"))\n",
        "        torch.save(D_A.state_dict(), os.path.join(SAVE_ROOT, f\"D_A_epoch{epoch}.pth\"))\n",
        "        torch.save(D_B.state_dict(), os.path.join(SAVE_ROOT, f\"D_B_epoch{epoch}.pth\"))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 494
        },
        "id": "-afW5rfqzGeo",
        "outputId": "1fa29fdd-6b68-410c-e4f5-fa4f52bda841"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/7.8 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.6/7.8 MB\u001b[0m \u001b[31m144.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m143.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m96.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for grad-cam (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/LeafGAN_masked_final_dataset_for_user/trainA'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2800592921.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     67\u001b[0m ])\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m train_dataset = LeafGANDataset(os.path.join(LEAFGAN_ROOT,\"trainA\"),\n\u001b[0m\u001b[1;32m     70\u001b[0m                                \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLEAFGAN_ROOT\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"trainB\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                                transform=transform)\n",
            "\u001b[0;32m/tmp/ipython-input-2800592921.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, rootA, rootB, transform)\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrootA\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrootA\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrootB\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrootB\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mA_files\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mf\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrootA\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".png\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\".jpg\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\".jpeg\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mB_files\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mf\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrootB\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".png\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\".jpg\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\".jpeg\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/LeafGAN_masked_final_dataset_for_user/trainA'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# inference cell\n",
        "import os, torch\n",
        "from torchvision import transforms, utils\n",
        "from PIL import Image\n",
        "from pathlib import Path\n",
        "# from your_model_defs import ResnetGenerator  # or copy generator class from training cell\n",
        "import torch.nn as nn # Import the nn module\n",
        "\n",
        "# ---------- Models ----------\n",
        "class ResnetBlock(nn.Module):\n",
        "    def __init__(self, dim):\n",
        "        super().__init__()\n",
        "        self.conv_block = nn.Sequential(\n",
        "            nn.ReflectionPad2d(1),\n",
        "            nn.Conv2d(dim, dim, 3),\n",
        "            nn.InstanceNorm2d(dim),\n",
        "            nn.ReLU(True),\n",
        "            nn.ReflectionPad2d(1),\n",
        "            nn.Conv2d(dim, dim, 3),\n",
        "            nn.InstanceNorm2d(dim),\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return x + self.conv_block(x)\n",
        "\n",
        "class ResnetGenerator(nn.Module):\n",
        "    def __init__(self, in_c=3, out_c=3, n_blocks=6, ngf=64):\n",
        "        super().__init__()\n",
        "        model = [nn.ReflectionPad2d(3),\n",
        "                 nn.Conv2d(in_c, ngf, 7), nn.InstanceNorm2d(ngf), nn.ReLU(True)]\n",
        "        # downsample\n",
        "        n_down = 2\n",
        "        curr = ngf\n",
        "        for i in range(n_down):\n",
        "            model += [nn.Conv2d(curr, curr*2, 3, 2, 1), nn.InstanceNorm2d(curr*2), nn.ReLU(True)]\n",
        "            curr *= 2\n",
        "        # resblocks\n",
        "        for _ in range(n_blocks):\n",
        "            model += [ResnetBlock(curr)]\n",
        "        # upsample\n",
        "        for i in range(n_down):\n",
        "            model += [nn.ConvTranspose2d(curr, curr//2, 3, 2, 1, output_padding=1), nn.InstanceNorm2d(curr//2), nn.ReLU(True)]\n",
        "            curr = curr//2\n",
        "        model += [nn.ReflectionPad2d(3), nn.Conv2d(curr, out_c, 7), nn.Tanh()]\n",
        "        self.model = nn.Sequential(*model)\n",
        "    def forward(self, x): return self.model(x)\n",
        "\n",
        "# ----------------- CONFIG -----------------\n",
        "LEAFGAN_ROOT = \"/content/LeafGAN_masked_final_dataset\"   # must contain trainA, trainB, testA, testB\n",
        "# ------------------------------------------\n",
        "\n",
        "\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "IMG_SIZE = 256\n",
        "SAVE_GEN_ROOT = \"/content/leafgan_results_2_synthetic/fakeB\"\n",
        "os.makedirs(SAVE_GEN_ROOT, exist_ok=True)\n",
        "\n",
        "# load trained generator (pick the checkpoint)\n",
        "ckpt = \"/content/leafgan_checkpoints_no_masks2/content/leafgan_checkpoints_no_masks2/G_AB_epoch50.pth\"  # change to your latest\n",
        "G_AB = ResnetGenerator().to(DEVICE)\n",
        "G_AB.load_state_dict(torch.load(ckpt, map_location=DEVICE))\n",
        "G_AB.eval()\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((IMG_SIZE,IMG_SIZE)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,0.5,0.5),(0.5,0.5,0.5))\n",
        "])\n",
        "\n",
        "src_dir = os.path.join(LEAFGAN_ROOT, \"trainA\")  # or testA / extra healthy images\n",
        "for fname in sorted(os.listdir(src_dir)):\n",
        "    if not fname.lower().endswith((\".png\",\".jpg\",\".jpeg\")): continue\n",
        "    img = Image.open(os.path.join(src_dir, fname)).convert(\"RGB\")\n",
        "    x = transform(img).unsqueeze(0).to(DEVICE)\n",
        "    with torch.no_grad():\n",
        "        fake = G_AB(x)\n",
        "    fake_img = (fake[0].cpu()*0.5 + 0.5).clamp(0,1)\n",
        "    utils.save_image(fake_img, os.path.join(SAVE_GEN_ROOT, fname))\n",
        "print(\"Saved generated images to:\", SAVE_GEN_ROOT)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2zUmMo4IAiPi",
        "outputId": "d77d5850-1f3d-4a3a-8df1-c3b88ce7b88d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved generated images to: /content/leafgan_results_2_synthetic/fakeB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# build_augmented.py\n",
        "import os, shutil\n",
        "LEAFGAN_ROOT = \"/content/LeafGAN_masked_final_dataset\"\n",
        "src_real_diseased = os.path.join(LEAFGAN_ROOT, \"trainB\")\n",
        "src_fake = \"/content/leafgan_results_2_synthetic/content/leafgan_results_2_synthetic/fakeB\"\n",
        "dst_aug = \"/content/Augmented_dataset_final/train/Diseased\"\n",
        "dst_healthy = \"/content/Augmented_dataset_final/train/Healthy\"\n",
        "os.makedirs(dst_aug, exist_ok=True)\n",
        "os.makedirs(dst_healthy, exist_ok=True)\n",
        "\n",
        "# copy healthy (real) to Healthy\n",
        "for f in os.listdir(os.path.join(LEAFGAN_ROOT, \"trainA\")):\n",
        "    if f.lower().endswith((\".png\",\".jpg\")):\n",
        "        shutil.copy(os.path.join(LEAFGAN_ROOT, \"trainA\", f), os.path.join(dst_healthy, f))\n",
        "\n",
        "# copy real diseased\n",
        "for f in os.listdir(src_real_diseased):\n",
        "    if f.lower().endswith((\".png\",\".jpg\")):\n",
        "        shutil.copy(os.path.join(src_real_diseased, f), os.path.join(dst_aug, f))\n",
        "\n",
        "# copy synthetic diseased\n",
        "for f in os.listdir(src_fake):\n",
        "    if f.lower().endswith((\".png\",\".jpg\")):\n",
        "        # rename if collision\n",
        "        destf = f\n",
        "        if os.path.exists(os.path.join(dst_aug, destf)):\n",
        "            destf = f.rsplit(\".\",1)[0] + \"_fake.\" + f.rsplit(\".\",1)[1]\n",
        "        shutil.copy(os.path.join(src_fake, f), os.path.join(dst_aug, destf))\n",
        "\n",
        "print(\"Augmented dataset ready at /content/Augmented_dataset_final/train\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MfVOyY-iCKPJ",
        "outputId": "bcd0092e-f435-4cd9-c883-ff5fdf8ed9cc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Augmented dataset ready at /content/Augmented_dataset_final/train\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Build Aug+RealBalanced (balanced 1:1)\n",
        "import os, shutil, random\n",
        "\n",
        "random.seed(42)\n",
        "\n",
        "LEAFGAN_ROOT = \"/content/LeafGAN_masked_final_dataset\"           # has trainA (healthy real), trainB (diseased real)\n",
        "SRC_FAKE     = \"/content/leafgan_results_2_synthetic/content/leafgan_results_2_synthetic/fakeB\"      # GAN-generated diseased\n",
        "DST          = \"/content/Augmented_dataset_AugPlusRealBalanced/train\"\n",
        "\n",
        "# fresh folders\n",
        "for cls in [\"Healthy\",\"Diseased\"]:\n",
        "    os.makedirs(os.path.join(DST, cls), exist_ok=True)\n",
        "\n",
        "# --- collect healthy (real) ---\n",
        "src_trainA = os.path.join(LEAFGAN_ROOT, \"trainA\")\n",
        "healthy_files = [f for f in os.listdir(src_trainA) if f.lower().endswith((\".jpg\",\".jpeg\",\".png\"))]\n",
        "\n",
        "# copy all real healthy\n",
        "for f in healthy_files:\n",
        "    shutil.copy(os.path.join(src_trainA, f), os.path.join(DST, \"Healthy\", f))\n",
        "\n",
        "# --- build diseased pool: real + synthetic ---\n",
        "src_trainB = os.path.join(LEAFGAN_ROOT, \"trainB\")\n",
        "real_dis = [os.path.join(src_trainB, f) for f in os.listdir(src_trainB) if f.lower().endswith((\".jpg\",\".jpeg\",\".png\"))]\n",
        "fake_dis = [os.path.join(SRC_FAKE, f)   for f in os.listdir(SRC_FAKE)   if f.lower().endswith((\".jpg\",\".jpeg\",\".png\"))]\n",
        "pool = real_dis + fake_dis\n",
        "random.shuffle(pool)\n",
        "\n",
        "# --- cap diseased to match healthy count (balanced 1:1) ---\n",
        "target = len(healthy_files)\n",
        "pool = pool[:target]\n",
        "\n",
        "# copy selected diseased; ensure unique filenames so we don't overwrite\n",
        "existing = set()\n",
        "for src in pool:\n",
        "    f = os.path.basename(src)\n",
        "    base, ext = os.path.splitext(f)\n",
        "    dest = os.path.join(DST, \"Diseased\", f)\n",
        "    i = 1\n",
        "    while os.path.exists(dest) or dest in existing:\n",
        "        dest = os.path.join(DST, \"Diseased\", f\"{base}_mix{i}{ext}\")\n",
        "        i += 1\n",
        "    shutil.copy(src, dest)\n",
        "    existing.add(dest)\n",
        "\n",
        "# --- verify counts ---\n",
        "h = len([x for x in os.listdir(os.path.join(DST,\"Healthy\")) if x.lower().endswith((\".jpg\",\".jpeg\",\".png\"))])\n",
        "d = len([x for x in os.listdir(os.path.join(DST,\"Diseased\")) if x.lower().endswith((\".jpg\",\".jpeg\",\".png\"))])\n",
        "print(\"✅ Balanced dataset built at:\", os.path.dirname(DST))\n",
        "print(\"Healthy:\", h, \" Diseased:\", d)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sJWFfyntuKBf",
        "outputId": "32bd5a5e-2196-4e6b-e9cc-14bba3785b14"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Balanced dataset built at: /content/Augmented_dataset_AugPlusRealBalanced\n",
            "Healthy: 1316  Diseased: 1316\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Build/confirm test set from the original LeafGAN dataset\n",
        "LEAFGAN = \"/content/LeafGAN_masked_final_dataset\"\n",
        "\n",
        "TEST_ROOT = \"/content/Classifier_test\"\n",
        "import os, shutil\n",
        "def reset(p):\n",
        "    if os.path.exists(p): shutil.rmtree(p)\n",
        "    os.makedirs(p, exist_ok=True)\n",
        "\n",
        "reset(TEST_ROOT); os.makedirs(f\"{TEST_ROOT}/Healthy\", exist_ok=True); os.makedirs(f\"{TEST_ROOT}/Diseased\", exist_ok=True)\n",
        "\n",
        "for f in os.listdir(f\"{LEAFGAN}/testA\"):\n",
        "    if f.lower().endswith((\".jpg\",\".png\",\".jpeg\")): shutil.copy(f\"{LEAFGAN}/testA/{f}\", f\"{TEST_ROOT}/Healthy/{f}\")\n",
        "for f in os.listdir(f\"{LEAFGAN}/testB\"):\n",
        "    if f.lower().endswith((\".jpg\",\".png\",\".jpeg\")): shutil.copy(f\"{LEAFGAN}/testB/{f}\", f\"{TEST_ROOT}/Diseased/{f}\")\n",
        "\n",
        "print(\"Test Healthy:\", len(os.listdir(f\"{TEST_ROOT}/Healthy\")))\n",
        "print(\"Test Diseased:\", len(os.listdir(f\"{TEST_ROOT}/Diseased\")))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yfDinNUR1n3B",
        "outputId": "9742ba40-9ca6-473a-e457-ed1440564893"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Healthy: 329\n",
            "Test Diseased: 125\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tXy9KeMv7cEa"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}